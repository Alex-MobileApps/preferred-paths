{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "a876d5d129613f84737ac732b9dceeab4d5b6e46c5cd6a0b580a7fe744c52632"
   }
  },
  "interpreter": {
   "hash": "a876d5d129613f84737ac732b9dceeab4d5b6e46c5cd6a0b580a7fe744c52632"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from brain import Brain\n",
    "from preferred_path import PreferredPath\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Normal\n",
    "from IPython import display\n",
    "from scipy.stats import norm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Global vars"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "res = 68 # Brain resolution\n",
    "fns = 4  # Number of criteria in path algorithm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train/test node pair splits"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_test_ind(res, train_pct=0.7):\n",
    "    rows = res * (res - 1)\n",
    "    rand = np.zeros((rows,2), dtype=np.int)\n",
    "    k = 0\n",
    "    for i in range(res-1):\n",
    "        for j in range(i+1, res):\n",
    "            rand[k] = (i,j)\n",
    "            rand[k+1] = (j,i)\n",
    "            k += 2\n",
    "    np.random.shuffle(rand)\n",
    "    train_rows = int(train_pct * rows)\n",
    "    train_ind, test_ind = rand[:train_rows,:], rand[train_rows:,:]\n",
    "    return (train_ind[:,0], train_ind[:,1]), (test_ind[:,0], test_ind[:,1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_ind, test_ind = train_test_ind(res)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Brain data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, sc, fc, euc_dist, hubs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        sc, fc : numpy.ndarray\n",
    "            Connectivity matrices for each subject\n",
    "            3D with shape: (number of subjects, resolution, resolution)\n",
    "        euc_dist : numpy.ndarray\n",
    "            Euclidean distance matrix\n",
    "            2D with shape: (resolution, resolution)\n",
    "        hubs : numpy.ndarray\n",
    "            Array with the indexes of the hub nodes\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(sc)\n",
    "        res = len(euc_dist)\n",
    "\n",
    "        # Init vars\n",
    "        triu = int(res * (res - 1) / 2)\n",
    "        triu_i = np.triu_indices(res, 1)\n",
    "        self.adj = np.zeros((n, triu))\n",
    "        self.sp = np.zeros((n, res, res))\n",
    "        self.pp = [None] * n\n",
    "\n",
    "        # Fill vars\n",
    "        for i in range(n):\n",
    "            brain = Brain(sc[i], fc[i], euc_dist, hubs=hubs)\n",
    "            streamlines = brain.streamlines()\n",
    "            node_str = brain.node_strength(weighted=False)\n",
    "            is_target = brain.is_target\n",
    "            is_hub = brain.hubs(binary=True)\n",
    "            fns = [\n",
    "                lambda loc, nxt, prev, target: streamlines[loc,nxt],\n",
    "                lambda loc, nxt, prev, target: node_str[nxt],\n",
    "                lambda loc, nxt, prev, target: is_target(nxt, target),\n",
    "                lambda loc, nxt, prev, target: is_hub[nxt]]\n",
    "            weights = list(np.random.random(size=len(fns)))\n",
    "            self.adj[i] = brain.sc_bin[triu_i]\n",
    "            self.sp[i] = brain.shortest_paths()\n",
    "            self.pp[i] = PreferredPath(adj=brain.sc_bin, fn_vector=fns, fn_weights=weights)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.adj)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.adj[idx], self.sp[idx], self.pp[idx])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_con(con, res, subj=None):\n",
    "    if subj is None:\n",
    "        subj = np.arange(1, 485)\n",
    "    mat_data = loadmat(f'data/subjfiles_{con}{res}.mat')\n",
    "    return np.array([mat_data[f's{str(z).zfill(3)}'] for z in subj])\n",
    "\n",
    "def load_data(res, subj=None):\n",
    "    sc = load_con('sc', res, subj)\n",
    "    fc = load_con('fc', res, subj)\n",
    "    euc_dist = loadmat('data/euc_dist.mat')[f'eu{res}']\n",
    "    hubs = np.loadtxt(f'data/hubs_{res}.txt', dtype=np.int, delimiter=',')\n",
    "    return BrainDataset(sc, fc, euc_dist, hubs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = load_data(res)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Policy estimator network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class PolicyEstimator():\n",
    "    def __init__(self, res, fn_len, hidden_units=20, load_path=None):\n",
    "        self.n_inputs = int(res * (res - 1) / 2)\n",
    "        self.n_outputs = fn_len * 2 # includes both mean and ln(sigma)\n",
    "        self.network = Sequential(\n",
    "            Linear(self.n_inputs, hidden_units),\n",
    "            ReLU(),\n",
    "            Linear(hidden_units, self.n_outputs))\n",
    "        if load_path:\n",
    "            self.network.load_state_dict(torch.load(load_path))\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.network(torch.FloatTensor(state))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pe = PolicyEstimator(res, fns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reward function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def reward(pred, sp):\n",
    "    inf_mask = np.isinf(pred)\n",
    "    r = 1 / (1 + pred - sp)\n",
    "    r[inf_mask] = -1\n",
    "    return r.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plotting helpers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cust_plot(ax, x, y, title=None, xlab=None, ylab=None, labels=None, off=0, avg=None):\n",
    "    inst = lambda M: any(isinstance(M, j) for j in [list, np.ndarray])\n",
    "    get = lambda M: (lambda i: M[i]) if inst(M[0])  else (lambda i: M)\n",
    "    len2d = lambda M: len(M) if inst(M[0]) else 1\n",
    "    getx = get(x)\n",
    "    gety = get(y)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlab)\n",
    "    ax.set_ylabel(ylab)\n",
    "    for i in range(len2d(y)):\n",
    "        yi = gety(i)[off:]\n",
    "        xi = getx(i)[off:]\n",
    "        label = labels[i] if labels else None\n",
    "        ax.plot(xi, yi, label=label)\n",
    "        if avg:\n",
    "             x_avg, y_avg = move_avg(yi, avg)\n",
    "             ax.plot(x_avg + xi[0], y_avg, label=f'{label} {avg} point avg')\n",
    "    if labels:\n",
    "        ax.legend()\n",
    "    return ax\n",
    "\n",
    "def cust_plot_pdf(ax, mu, sig, title=None, xlab=None, ylab=None, labels=None):\n",
    "    xmin = (mu - 3 * sig).min()\n",
    "    xmax = (mu + 3 * sig).max()\n",
    "    x = np.arange(xmin, xmax, 0.001)\n",
    "    y = [norm.pdf(x, mu[i], sig[i]) for i in range(len(mu))]\n",
    "    return cust_plot(ax=ax, x=x, y=y, title=title, xlab=xlab, ylab=ylab, labels=labels)\n",
    "\n",
    "def move_avg(y, p):\n",
    "    if len(y) < p:\n",
    "        return np.array([]), np.array([])\n",
    "    c = np.cumsum(y)\n",
    "    y_avg = (c[p:] - c[:-p]) / p\n",
    "    x_avg = np.arange(len(y_avg)) + p\n",
    "    return x_avg, y_avg\n",
    "\n",
    "def plot_all(batch, plt_avg=None, plt_off=0, figsize=(20,30)):\n",
    "    _, ax = plt.subplots(nrows=4, ncols=2, figsize=figsize, facecolor='w')\n",
    "    len_plt_rewards = len(plt_train_rewards)\n",
    "    len_data = len(data)\n",
    "    x = np.arange(len_plt_rewards) + 1\n",
    "    fn_labs = [f'fn{j+1}' for j in range(fns)]\n",
    "    def_plot = lambda ax, y, ylab, labels, title, avg=None: cust_plot(ax, x, y, xlab='Batches', ylab=ylab, labels=labels, off=plt_off, title=f'{title}\\n(n={len_data}, res={res}, batch size={batch})', avg=avg)\n",
    "    def_plot(ax[0,0], plt_train_rewards,   ylab='Mean rewards (Train)',  labels=['Train'], title='Mean rewards (Train) vs. batches',                           avg=plt_avg)\n",
    "    def_plot(ax[0,1], plt_test_rewards,    ylab='Mean rewards (Test)',   labels=['Test'],  title='Mean rewards vs. batches',                           avg=plt_avg)\n",
    "    def_plot(ax[1,0], plt_train_success,   ylab='Success ratio (Train)', labels=['Train'], title='Success ratio (Train) vs. batches',                          avg=plt_avg)\n",
    "    def_plot(ax[1,1], plt_test_success,    ylab='Success ratio (Test)',  labels=['Test'],  title='Success ratio vs. batches',                          avg=plt_avg)\n",
    "    def_plot(ax[2,0], plt_mu,              ylab='Mu',                    labels=fn_labs,   title='Mean criteria weight vs. batches')\n",
    "    def_plot(ax[2,1], plt_sig,             ylab='Sigma',                 labels=fn_labs,   title='Standard deviation for criteria weight vs. batches')\n",
    "    cust_plot_pdf(ax[3,0], np.array(plt_mu)[:,-1], np.array(plt_sig)[:,-1], xlab='Weight', ylab='Density', labels=fn_labs,\n",
    "        title=f\"Probility density function for criteria weights (n={len_data}, res={res}, batch={batch})\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforce"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plotting data\n",
    "\n",
    "plt_train_rewards = []\n",
    "plt_test_rewards = []\n",
    "plt_train_success = []\n",
    "plt_test_success = []\n",
    "plt_mu = [[] for _ in range(fns)]\n",
    "plt_sig = [[] for _ in range(fns)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def reinforce(pe, data, epochs, batch=22, lr=0.01, plot=True, print_freq=22, plt_off=0, plt_avg=None): # batch: 2,4,11,22,44,121,242 divide 484\n",
    "    # Setup\n",
    "    opt = Adam(pe.network.parameters(), lr=lr)\n",
    "    len_data = len(data)\n",
    "    len_fn = data.pp[0].fn_length\n",
    "\n",
    "    # Run\n",
    "    for e in range(epochs):\n",
    "        print(f'(Epoch {e+1}):', end=' ')\n",
    "        offset = 0\n",
    "\n",
    "        # Epoch\n",
    "        while offset + batch <= len_data:\n",
    "            train_rewards = torch.zeros(batch,1)\n",
    "            test_rewards = torch.zeros(batch, 1)\n",
    "            train_success = np.zeros(batch)\n",
    "            test_success = np.zeros(batch)\n",
    "            adj, sp, pp = data[offset:offset+batch]\n",
    "            probs = pe.predict(adj)\n",
    "            mu, sig = probs[:,:len_fn], torch.exp(probs[:,len_fn:])\n",
    "            m = Normal(mu, sig)\n",
    "            actions = m.sample()\n",
    "\n",
    "            # Batch\n",
    "            for i in range(batch):\n",
    "                print(i+1+offset, end=' ')\n",
    "                pp[i].fn_weights = actions[i].tolist()\n",
    "                pred = pp[i].retrieve_all_paths()\n",
    "\n",
    "                # Train\n",
    "                train_pred = pred[train_ind]\n",
    "                train_sp = sp[i][train_ind]\n",
    "                train_mask = np.where((train_sp > 0) & (~np.isinf(train_sp)))\n",
    "                train_rewards[i] = reward(train_pred[train_mask], train_sp[train_mask])\n",
    "                train_success[i] = 1 - np.isinf(train_pred).sum() / len(train_pred)\n",
    "\n",
    "                # Test\n",
    "                test_pred = pred[test_ind]\n",
    "                test_sp = sp[i][test_ind]\n",
    "                test_mask = np.where((test_sp > 0) & (~np.isinf(test_sp)))\n",
    "                test_rewards[i] = reward(test_pred[test_mask], test_sp[test_mask])\n",
    "                test_success[i] = 1 - np.isinf(test_pred).sum() / len(test_pred)\n",
    "\n",
    "            # Step\n",
    "            opt.zero_grad()\n",
    "            loss = -m.log_prob(actions) * train_rewards\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # Plotting\n",
    "            if plot:\n",
    "                # Add data to arrays\n",
    "                plt_train_rewards.append(train_rewards.mean().item())\n",
    "                plt_test_rewards.append(test_rewards.mean().item())\n",
    "                plt_train_success.append(train_success.mean())\n",
    "                plt_test_success.append(test_success.mean())\n",
    "                for j in range(len_fn):\n",
    "                    plt_mu[j].append(mu[:,j].mean().item())\n",
    "                    plt_sig[j].append(sig[:,j].mean().item())\n",
    "                len_plt_rewards = len(plt_train_rewards)\n",
    "\n",
    "                # Plot data\n",
    "                if (len_plt_rewards + 1) % print_freq == 0:\n",
    "                    plot_all(batch, plt_avg, plt_off)\n",
    "                    display.clear_output(wait=True)\n",
    "                    display.display(plt.gcf())\n",
    "\n",
    "            # Run next batch\n",
    "            offset += batch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reinforce(pe, data, epochs=100, batch=1, lr=0.005, print_freq=484, plt_off=0, plt_avg=200)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_all(batch=1, plt_avg=None, plt_off=0, figsize=(30,30))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def save(fname_prefix):\n",
    "    np_save = lambda ext, A: np.savetxt(f'{fname_prefix}_{ext}.txt', A, fmt='%.18f', delimiter=',')\n",
    "    np_save('train_rewards', plt_train_rewards)\n",
    "    np_save('test_rewards',  plt_test_rewards)\n",
    "    np_save('train_success', plt_train_success)\n",
    "    np_save('test_success',  plt_test_success)\n",
    "    np_save('mu',  plt_mu)\n",
    "    np_save('sig',  plt_sig)\n",
    "    torch.save(pe.network.state_dict(), f'{fname_prefix}_torch_state.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#save('results/s001')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}